{"/blog":{"title":"Blog","data":{"":"Blog\nThe latest updates from Falcon. See Changelog for more product\nupdates."}},"/blog/update-2023-11":{"title":"Falcon Update ‚Äî November 2023","data":{"":"Hello everyone üëã, in the past month we've concentrated on enhancing integrations and adding more trace context capabilities to Falcon.\nOptimized trace data ingestion\nLangchain compatibility for JavaScript/TypeScript developers\nImplementation of Langchain within a Trace context\nMonitoring of software releases and iterations in Python and JavaScript/TypeScript\nAdvanced filtering on the Traces table\nAdvanced Analytics Features\nUSD cost computation for token usage\nToken utilization charts\nAdditional Updates\nGET API enhancements: filtering by user, model, and date; access to unprocessed traces\nStreamlined self-hosting process with Docker\n... along with numerous incremental improvements and bug fixes.Detailed Breakdown üëá","langchain#ü¶úüîó JavaScript/TypeScript Langchain Integration":"Following the release of the Python Integration for Langchain, we're excited to introduce the equivalent for JavaScript/TypeScript teams. Our new package, falcon-langchain, includes a CallbackHandler that seamlessly integrates complex Langchain sequences and agents into your tracing. Simply incorporate it as a callback.\n// Initialize Falcon Callback Handler\nimport CallbackHandler from \"falcon-langchain\";\nconst handler = new CallbackHandler({\n  secretKey: process.env.FALCON_SECRET_KEY, // sk-lf-...\n  publicKey: process.env.FALCON_PUBLIC_KEY, // pk-lf-...\n  // additional options\n});\n// Configure Langchain\nimport { OpenAI } from \"langchain/llms/openai\";\nconst llm = new OpenAI();\n// Integrate Falcon Callback Handler\nconst res = await llm.call(\"<user-input>\", { callbacks: [handler] });\n‚Üí Integration Documentation\n‚õìÔ∏è Enhanced Trace Context with Langchain Integrations [#langchain-trace]\nWith the Langchain Python integration, you can now introduce additional context to the traces. This allows for the inclusion of user IDs, metadata, or customized identifiers to associate evaluations with the trace directly.\npython\nCopy code\nimport uuid\nfrom falcon.client import Falcon\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n# Initialize Falcon client\nfalcon = Falcon(ENV_PUBLIC_KEY, ENV_SECRET_KEY, ENV_HOST)\n# Generate a unique trace_id\ntrace_id = str(uuid.uuid4())\n# Establish the Trace\ntrace = falcon.trace(id=trace_id)\n# Generate a handler associated with the Trace\nhandler = trace.getNewHandler()\n# Set up Langchain\nllm = OpenAI()\nchain = LLMChain(llm=llm, prompt=PromptTemplate(...))\nchain.run(\"<your-input>\", callbacks=[handler])\n‚Üí Documentation\nüì¶ Incorporating Trace Context: Tracking Releases and Versions [#releases-and-versions]"}},"/blog/update-2023-12":{"title":"Falcon Update ‚Äî December 2023","data":{"":"Hello everyone üëã, let's dive into a summary of the key updates we rolled out in September:\nEvaluations based on model functionality through Python SDK\nData assemblies (beta) for compiling inputs and expected results within Falcon to assess your LLM application\nBuilt-in analytics energized by a new query engine\nBrand-new integrations with Flowise, Langflow, and LiteLLM\nRefinements\nAdvanced filtering across all data grids\nPublic sharing of traces via URL\nData extraction as CSV, JSON, or JSONL (useful for model refinement)\n... plus a suite of minor enhancements and bug corrections.In-depth Information üëá","model-based-eval#üö¶ Model Functionality Evaluations via Python SDK":"We've crafted a sample setupto demonstrate the execution of model functionality evaluations on operational data in Falcon by using the Python SDK.The novel get_generations function empowersyou to retrieve all generated outputs based on specific criteria (such as a particular name). Afterwards, you can apply your evaluation function to each output and append the resulting scores to Falcon for further analysis.This enables you to apply your preferred evaluation tools (like OpenAI evaluations, Langkit, Langchain) to all generated content within Falcon.\nfrom falcon import Falcon\nfalcon = Falcon(LF_PUBLIC_KEY, LF_SECRET_KEY)\ngenerations = falcon.get_generations(name=\"my_model_eval\").data\nfor generation in generations:\n    # Import function from an evaluation library, refer to the documentation for more details\n    eval_result = my_custom_eval(\n      generation.prompt,\n      generation.completion\n    )\n    falcon.add_score(\n      ScoreEntry(\n        name=\"accuracy\",\n        traceId=generation.trace_id,\n        observationId=generation.id,\n        score=eval_result[\"evaluation_score\"],\n        remarks=eval_result['justification']\n      )\n    )\n‚Üí Evaluation Documentation\nüóÇÔ∏è Data Collections (Beta) [#datasets]\nIntroducing data collections for beta testing in Falcon. These are compilations of inputs and expected responses that you can oversee in Falcon. You can either upload a pre-existing data collection or formulate one from operational data (like when you stumble upon new edge cases).\nWhen paired with automated evaluations, Falcon's data collections facilitate systematic assessments of new iterations for your LLM application.\nInsight into data collection runs on a demo set\nPerforming an experiment on a data collection\n<Tabs items={[\"Python\", \"JS/TS\", \"Langchain (Python)\"]} storageKey=\"data_collections\">\n<Tab>\npython\nCopy code\ndata_collection = falcon.get_data_collection(\"<data_collection_name>\")\nfor item in data_collection.entries:\n    # Execute your LLM application function and obtain the parent observation in Falcon (span/generation/event)\n    # The output is also captured as it's required for the evaluation of the run\n    generated_output, result = my_llm_app.execute(item.prompt)\n    # Connect the execution trace to the data collection entry and label it with a run identifier\n    item.assign_trace(generated_output, \"<run_identifier>\")\n    # Optionally, compute an evaluation score for the output to streamline comparison between different iterations\n    generated_output.evaluate(\n      name=\"evaluation_metric\",\n      # Any numerical score\n      score=my_evaluation_function(\n          item.prompt,\n          result,\n          item.expected_response\n      )\n    )\n</Tab>\n<Tab>\ntypescript\nCopy code\nconst dataCollection = await falcon.getDataCollection(\"<data_collection_name>\");\nfor (const entry of dataCollection.entries) {\n  // Execute your LLM application function and obtain the Falcon parent observation (span/generation/event)\n  // The output is also returned as it's needed for evaluating the run\n  const [generatedOutput, result] = await myLlmApp.execute(entry.prompt);\n  // Link the execution trace to the data collection entry and assign a run identifier\n  await entry.assignTrace(generatedOutput, \"<run_identifier>\");\n  // If desired, assess the output to aid in comparing different iterations\n  generatedOutput.evaluate({\n    name: \"evaluation_metric_name\",\n    score: myEvaluationFunction(entry.prompt, result, entry.expectedResponse),\n  });\n}\n</Tab>\n<Tab>\npython\nCopy code\ndata_collection = falcon.get_data_collection(\"<data_collection_name>\")\nfor entry in data_collection.entries:\n    # Langchain callback handler automatically connects the execution trace to the data collection entry\n    handler = entry.get_langchain_handler(run_identifier=\"<run_identifier>\")\n    # Execute the application and pass the custom handler\n    my_langchain_workflow.execute(entry.prompt, callbacks=[handler])\n</Tab>\n</Tabs>\nCurrently in beta on Falcon Cloud, the data collections API might undergo minor adjustments. If you're interested in trying it out, please reach out via the in-app messenger.\n‚Üí Data Collections Documentation\n‚Üí Python Data Collection Guide\nüìä Built-in Dashboards [#analytics]\nIn recent weeks, analytics capabilities were in public alpha on Falcon Cloud. We've just introduced a new query engine which serves as the foundation for the in-app dashboards. This is a crucial development that integrates all analytics functionalities into the Falcon core, enabling more rapid progression.\nYou'll notice an influx of new dashboards in the application soon."}},"/blog/update-2024-01":{"title":"Falcon Update ‚Äî Jan 2024","data":{"":"Greetings everyone üëã, take a moment to explore the latest features introduced in Januar:\nEnhanced interface\nIntegration with OpenAI SDK (Python)\nGuidebook for RAG evaluations in collaboration with Ragas\nAPI performance enhancement\nStreamlined Docker setup\nSDK enhancements\nSDKs now reveal trace URL\nPython SDK compatibility with Pydantic versions 1 and 2\nUser experience enhancements\nEnhanced navigation through traces\nSingle Sign-On via Google and GitHub\nProject deletion feature\nDocumentation enhancements: Instantly access all Python documentation as Jupyter notebooks\n... accompanied by various minor enhancements and bug resolutions.Detailed Insights üëá","dashboard#üìà Dashboard Enhancements":"New additions include\nCost analysis per model\nLatency distributions per model (50th, 90th, 95th, 99th percentiles)\nEnd-user cost and usage patterns\nTrace type usage for specific application scenarios\nEnhanced date selection tool","openai#ü§ñ OpenAI SDK Compatibility":"Seamlessly switch to Falcon's Python SDK for complete insight by adjusting a single line. Auto-captures: prompts, outputs, utilization, timing, and API discrepancies."}},"/cookie-policy":{"title":"Cookie Policy","data":{}},"/docs/demo":{"title":"Interactive Demo","data":{"":"Experience Falcon firsthand with a project you can view but not edit.","explore-the-demo-project-in-falcon#Explore the Demo Project in Falcon":"Secure your spot in the demo project by signing up for a free account. No credit card necessary.","generate-new-traces--provide-feedback#Generate New Traces & Provide Feedback":"Engage with the chatbot presented below to observe new traces and submit your feedback (üëç/üëé) directly within Falcon."}},"/":{"title":"Falcon","data":{}},"/pricing":{"title":"Pricing","data":{}},"/tos":{"title":"Terms of Service","data":{}},"/privacy":{"title":"Privacy Policy","data":{}},"/oss-friends":{"title":"OSS Friends","data":{"":"We are proud to collaborate with a diverse group of partners to promote open-source software and the values of transparency, collaboration, and community that it represents.Learn more about our commitment to open source.","our-friends#Our friends":"","become-a-friend#Become a friend":"Learn more about OSS Friends and how to become one here."}}}